import json
import os
import re

import gradio as gr
import dotenv
from fireworks.client import Fireworks

models = {"Llama 3.1 8B (Finetuned for tagging)": "accounts/d0nnw0n9-c1910b/models/finer",
          "Llama 3.1 8B (Finetuned for extraction)": "accounts/d0nnw0n9-c1910b/models/extraction",
          "Llama 3.1 8B (Base)": "accounts/fireworks/models/llama-v3p1-8b-instruct"}


def inference(inputs: str, model, max_new_token=15, delimiter="\n", if_print_out=False):
    config = 0
    try:
        config = dotenv.dotenv_values("../.env")['FIREWORKS_KEY']
    except:
        try:
            config = os.getenv('FIREWORKS_KEY')
        except:
            pass

    client = Fireworks(api_key=config)
    response = client.chat.completions.create(
        model=model,
        max_tokens=max_new_token,
        messages=[
            {
                "role": "user",
                "content": inputs
            }
        ],
        stream=False
    )
    answer = (response.choices[0].message.content)
    # print(answer)
    return answer


def process_tagging(sentence):
    numbers = re.findall(r'\b\d+\.?\d*\b', sentence)
    months = ["January", "February", "March", "April", "May", "June",
              "July", "August", "September", "October", "November", "December"]

    extracted_numbers = []
    for num_str in numbers:
        if num_str in [str(x) for x in list(range(2000, 2025, 1))]:
            continue

        # Exclude 1 or 2 digit numbers followed by a comma and then a 4 digit number (likely day and year)
        match = re.search(rf"{re.escape(num_str)}\s*,\s*\d{{4}}", sentence)
        if match:
            continue

        # Exclude numbers followed by a month
        match = re.search(rf"{re.escape(num_str)}\s+({'|'.join(months)})", sentence, re.IGNORECASE)
        if match:
            continue

        extracted_numbers.append(num_str)
    print(extracted_numbers)

    result = [[], []]

    for i, model in enumerate(
            ["accounts/fireworks/models/llama-v3p1-8b-instruct", "accounts/d0nnw0n9-c1910b/models/finer"]):
        for x in extracted_numbers:
            prompt = f'''What is the appropriate XBRL US GAAP tag for "{x}" in the given sentence? Output the US GAAP tag only and nothing else. \n "{sentence}"\n'''
            output = inference(prompt, model)
            output = output.split("<|end_of_text|>")[0]
            result[i].append([x, output])

    gt = None
    if sentence in tagging_example:
        gt = tagging_example[sentence]
    return result[0], result[1], gt


def process_extract(question, file):
    if file not in extraction_data:
        raise gr.Error("This XBRL file does not exist. Please select a valid file name from the examples", duration=5)

    if question in extraction_data[file]:
        gt_answer = extraction_data[file][question]['target']
        context = extraction_data[file][question]['context'].replace("QQQQQ", question)
    else:
        gt_answer = None
        context = list(extraction_data[file].values())[0]['context'].replace("QQQQQ", question)

    result = [[], []]
    for i, model in enumerate(
            ["accounts/fireworks/models/llama-v3p1-8b-instruct", "accounts/d0nnw0n9-c1910b/models/extraction"]):
        output = inference(context, model)
        result[i] = output.split("<|end_of_text|>")[0]

    return result[0], result[1], gt_answer


if __name__ == '__main__':
    with open('finer_example.json') as f:
        tagging_example = json.load(f)
    with open('extraction_example.json') as f:
        extraction_data = json.load(f)

    extraction_example = []
    for f in extraction_data:
        for x in extraction_data[f]:
            extraction_example.append([x, f])

    with gr.Blocks() as tagging:
        gr.Markdown("""
## XBRL Tagging 

* **Input:** Provide a sentence containing financial information.
* **Output:** Key entities and their corresponding US GAAP (Generally Accepted Accounting Principles) tags will be generated by the base model and our fine-tuned model.

Feel free to explore the examples below or enter your own sentence.
""")
        gr.Interface(
            cache_examples=False,
            examples_per_page=20,
            fn=process_tagging,
            inputs=[
                gr.Textbox(label="Sentence"),
            ],
            outputs=[gr.Dataframe(label="Llama 3.1 8b (base) output", headers=["Entites", "US GAAP tags"]),
                     gr.Dataframe(label="Llama 3.1 8b (fine-tuned for XBRL tagging) output",
                                  headers=["Entites", "US GAAP tags"]),
                     gr.Dataframe(label="Ground Truth Answer", headers=["Entites", "US GAAP tags"])],
            examples=[[x] for x in tagging_example.keys()]
        )
    with gr.Blocks() as extraction:
        gr.Markdown(
            """
## XBRL Extraction

* **Input:** A financial question and an XBRL file name.

* **Output:** The answer to the question will be generated by the base model and our fine-tuned model.

Feel free to explore the examples below or enter your own question.
            """
        )
        gr.Interface(
            fn=process_extract,
            cache_examples=False,
            inputs=[
                gr.Textbox(label="Question"),
                gr.Textbox(label="XBRL File Name"),
            ],
            outputs=[gr.Text(label="Llama 3.1 8b (Base) output"),
                     gr.Text(label="Llama 3.1 8b (fine-tuned for XBRL extraction) output"),
                     gr.Textbox(label="Ground truth answer")],
            examples=extraction_example,
            examples_per_page=20,
        )

    with gr.Blocks(theme=gr.themes.Soft(font=[gr.themes.GoogleFont("IBM Plex Sans"), "system-ui", "sans-serif"])) as demo:
        gr.Markdown("# XBRL Enhanced LLM Demo")
        gr.TabbedInterface([tagging, extraction], ["XBRL Tagging", "XBRL Extraction"])

    demo.launch(share=True)
